<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0086)http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/classification.html -->
<html class="gr__cs_rpi_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document"><title>Project 5: Classification</title>

<link href="./Project 5_ Classification_files/projects.css" rel="stylesheet" type="text/css"></head>

<body data-gr-c-s-loaded="true">
<h2>Acknowledgements</h2>
This project is part of the <a href="http://ai.berkeley.edu/project_overview.html">Pac-man projects</a> created by <a href="http://www.denero.org/">John DeNero</a> and <a href="http://www.eecs.berkeley.edu/~klein">Dan Klein</a> for <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">CS188</a> at Berkeley EECS. We thank <a href="http://www.cs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>, <a href="http://www.denero.org/">John DeNero</a>, and <a href="http://www.eecs.berkeley.edu/~klein">Dan Klein</a> for sharing it with us and allowing us to use as course project.
<h2>Project 5: Classification</h2>
<table border="0" cellpadding="10">
<tbody><tr>
<td align="center">
  <img src="./Project 5_ Classification_files/img2.gif"> <br>
  Which Digit?
</td>
<td></td>
<td align="center">
  <table border="0" cellpadding="4">
  <tbody><tr>
    </tr><tr>
      <td><img src="./Project 5_ Classification_files/i1.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/image_0056.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i2.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/image_0067.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/image_0332.jpg" width="60"></td>
    </tr>
    <tr>
      <td><img src="./Project 5_ Classification_files/image_0128.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i3.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/image_0193.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i4.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i5.jpg" width="60"></td>
    </tr>
    <tr>
      <td><img src="./Project 5_ Classification_files/i6.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i7.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/image_0196.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i8.jpg" width="60"></td>
      <td><img src="./Project 5_ Classification_files/i9.jpg" width="60"></td>
    </tr>
  </tbody></table>
  Which are Faces?
</td>
</tr>
</tbody></table>

<br>
<br>
<em>Due 12/09 at 11:59pm</em>

<h2>Introduction</h2>
<p>In this project, you will design three classifiers: a naive Bayes classifier, a perceptron 
classifier and a large-margin (MIRA) classifier. You will test your classifiers on two 
image data sets: a set of scanned handwritten digit images and a set of face images 
in which edges have already been detected. Even with simple features, your classifiers will 
be able to do quite well on these tasks when given enough training data.


</p><p>Optical character recognition (<a href="http://en.wikipedia.org/wiki/Optical_character_recognition">OCR</a>) 
is the task of extracting text from image sources. The first
data set on which you will run your classifiers is a collection of handwritten numerical digits (0-9). 
This is a very commercially useful technology, similar to the technique used by the 
US post office to route mail by zip codes. 
There are systems that can perform with over 99% classification accuracy 
(see <a href="http://yann.lecun.com/exdb/lenet/index.html">LeNet-5</a> for an example system in action).

</p><p>Face detection is the task of localizing faces within video or still images.  The faces can be at any 
location and vary in size. There are many applications for face detection, including human computer 
interaction and surveillance. You will attempt a simplified face detection task in which your system is 
presented with an image that has been pre-processed by an edge detection algorithm.  The task is 
to determine whether the edge image is a face or not. There are several systems in use that perform 
quite well at the face detection task. One good system is the 
<a href="http://vasc.ri.cmu.edu/NNFaceDetector/">Face Detector</a> by Schneiderman and Kanade. 
You can even try it out on your own photos in this <a href="http://demo.pittpatt.com/">demo</a>.

</p><p>The code for this project includes the following files and data, available as a 
<a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/classification.zip">zip file</a>.&nbsp;</p> 
<table border="0" cellpadding="5">
  <tbody>
  <tr>
      <td colspan="2"><h5>Data file</h5></td>
  </tr>
  <tr>
    <td><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/data.zip"><code>data.zip</code></a></td>
    <td>Data file, including the digit and face data. </td>
  </tr>
  
  <tr>
      <td colspan="2"><h5>Files you will edit</h5></td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/naiveBayes.html">naiveBayes.py</a></code></td>
    <td>The location where you will write your naive Bayes classifier.</td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code></td>
    <td>The location where you will write your perceptron classifier. </td>
  </tr>
  <tr>
      <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/mira.html">mira.py</a></code></td>
      <td>The location where you will write your MIRA classifier. </td>
    </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/dataClassifier.html">dataClassifier.py</a></code></td>
    <td>The wrapper code that will call your classifiers. You will also write your 
    enhanced feature extractor here. You will also use this code to analyze the behavior of your
      classifier. </td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/answers.html">answers.py</a></code></td>
    <td>Answers to Question 2 and Question 4 go here.</td>
  </tr>

  <tr>
        <td colspan="2"><h5>Files you should read but NOT edit</h5></td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/classificationMethod.html">classificationMethod.py</a></code></td>
    <td>Abstract super class for the classifiers you will write. <br>(You <b>should</b> read this file carefully to see how the infrastructure is set up.)</td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/samples.html">samples.py</a></code></td>
    <td>I/O code to read in the classification data.  </td>
  </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/util.html">util.py</a></code></td>
    <td>Code defining some useful tools.  You may be familiar with some of these by now, and 
    they will save you a lot of time.
    </td> </tr>
  <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/mostFrequent.html">mostFrequent.py</a></code></td>
    <td>A simple baseline classifier that just labels every instance as the most frequent class. </td>
  </tr>
    <tr>
    <td><code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/runMinicontest.html">runMinicontest.py</a></code></td>
    <td>The command you will use to run the minicontest, if you decide to enter.</td>
  </tr>

</tbody></table>
<p>
</p>
<p><strong>What to submit:</strong> You will fill in portions of <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/answers.html">answers.py</a></code>, <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/naiveBayes.html">naiveBayes.py</a></code>,
<code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code>, <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/mira.html">mira.py</a></code> and <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/dataClassifier.html">dataClassifier.py</a></code>
(only) during the assignment, and submit them.</p>

<p><strong>Evaluation:</strong> Your code will be autograded for technical
correctness. Please <em>do not</em> change the names of any provided functions
or classes within the code, or you will wreak havoc on the autograder.
</p>

<p><strong>Academic Dishonesty:</strong> We will be checking your code against
other submissions in the class for logical redundancy. If you copy someone
else's code and submit it with minor changes, we will know. These cheat
detectors are quite hard to fool, so please don't try. We trust you all to
submit your own work only; please don't let us down. Instead, contact the course
staff if you are having trouble.

</p><h2>Getting Started</h2>
<p> To try out the classification pipeline, run <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/dataClassifier.html">dataClassifier.py</a></code> 
from the command line. This 
will classify the digit data using the default classifier (<code>mostFrequent</code>) which blindly classifies every example
with the most frequent label.

</p><pre>python dataClassifier.py  </pre>

<p>As usual, you can learn more about the possible command line options by running: 
    
</p><pre>python dataClassifier.py -h  </pre>

<p> We have defined some simple features for you. 
Later you will design some better features. Our simple feature set includes one feature for
each pixel location, which can take values 0 or 1 (off or on). The features are encoded as 
a <code>Counter</code> where keys are feature locations (represented as (column,row)) and 
values are 0 or 1. The face recognition data set has value 1 only for those pixels identified 
by a Canny edge detector.

</p><p> Implementation Note: You'll find it easiest to hard-code the binary feature assumption. 
If you do, make sure you don't include any non-binary features. Or, you can write your code
more generally, to handle arbitrary feature values, though this will probably involve
a preliminary pass through the training set to find all possible feature values (and you'll
need an "unknown" option in case you encounter a value in the test data you never saw
during training).

</p><h2>Naive Bayes</h2>

<p> A skeleton implementation of a naive Bayes classifier is provided for you in 
<code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/naiveBayes.html">naiveBayes.py</a></code>. 
You will fill in the <code>trainAndTune</code> function, the 
<code>calculateLogJointProbabilities</code> function and the 
<code>findHighOddsFeatures</code> function.

</p><h4>Theory</h4>

<p>A naive Bayes classifier
 models a joint distribution over a label <img width="17" height="14" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img1.png" alt="$Y$"> and a set of observed random variables, or <i>features</i>, 
<img height="18" align="TOP" border="0" src="./Project 5_ Classification_files/img2.png" alt="$\{F_1, F_2, \ldots F_n\}$">,
 using the assumption that the full joint distribution can be factored as follows (features are conditionally independent given the label):
 <br></p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img3.png" alt="\begin{displaymath}
P(F_1 \ldots F_n, Y) = P(Y) \prod_i P(F_i \vert Y)
\end{displaymath}">
</div>
<br clear="ALL">
<p></p>

<p>
To classify a datum, we can find the most probable label given the feature values for each pixel, using Bayes theorem:
</p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img4_new.png" alt="\begin{eqnarray*}
P(y \vert f_1, \ldots, f_m) &amp;=&amp; \frac{P(f_1, \ldots, f_m \...
...
&amp;=&amp; \textmd{arg max}_{y} P(y) \prod_{i = 1}^m P(f_i \vert y)
\end{eqnarray*}"></div>
<br clear="ALL"><p></p>

<p>
Because multiplying many probabilities together often results in underflow, we will instead compute <em><b>log
probabilities</b></em> which have the same argmax:
</p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img5.png" alt="\begin{eqnarray*}
\textmd{arg max}_{y} log(P(y \vert f_1, \ldots, f_m) &amp;=&amp; \te...
...{arg max}_{y} (log(P(y)) + \sum_{i = 1}^m log(P(f_i \vert y)))
\end{eqnarray*}"></div>
<br clear="ALL"><p></p>



<p> To compute logarithms, use <code>math.log()</code>, a built-in Python function.

</p><h4>Parameter Estimation</h4>
Our naive Bayes model has several parameters to estimate.  One
parameter is the <em><b>prior distribution</b></em> over labels (digits, or face/not-face),
<img width="42" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img6.png" alt="$P(Y)$">.

<p>
We can estimate <img width="42" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img6.png" alt="$P(Y)$"> directly from the training data:
<br></p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img7.png" alt="\begin{displaymath}
\hat{P}(y) = \frac{c(y)}{n}
\end{displaymath}">
</div>
<br clear="ALL">
<p></p>
where <img width="32" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img8.png" alt="$c(y)$"> is the number of training instances with label y and
n is the total number of training instances.

<p>
The other parameters to estimate are the <em><b>conditional probabilities</b></em> of
our features given each label y: 
<img width="92" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img9.png" alt="$P(F_i \vert Y = y)$">. We do this for each
possible feature value (<img height="18" align="TOP" src="./Project 5_ Classification_files/img10.png" alt="$f_i \in {0,1}$">).
</p><p></p>
<div align="CENTER">
<a name="empirical"></a><img src="./Project 5_ Classification_files/img11.png" alt="\begin{eqnarray*}
\hat{P}(F_i=f_i\vert Y=y) &amp;=&amp; \frac{c(f_i,y)}{\sum_{f_i}{c(f_i,y)}} \\
\end{eqnarray*}"></div>
<br clear="ALL"><p></p>
where <img width="52" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img12.png" alt="$c(f_i,y)$"> is the number of times pixel <img width="20" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img13.png" alt="$F_i$"> took value <img width="18" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img14.png" alt="$f_i$">
in the training examples of label y.

<p></p><h4>Smoothing</h4>
Your current parameter estimates are <i>unsmoothed</i>, that is, you are
using the empirical estimates for the parameters <img width="55" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img15.png" alt="$P(f_i\vert y)$">.  These
estimates are rarely adequate in real systems.  Minimally, we need to
make sure that no parameter ever receives an estimate of zero, but
good smoothing can boost accuracy quite a bit by reducing
overfitting.

<p>
In this project, we use <em>Laplace smoothing</em>, which adds <em>k</em> counts to every possible observation value:
</p><p>
</p><div align="CENTER">    
<img src="./Project 5_ Classification_files/imgsmoothlaplace.png" alt="$P(F_i=f_i\vert Y=y) = \frac{c(F_i=f_i,Y=y)+k}{\sum_{f_i}{(c(F_i=f_i,Y=y)+k)}}$">
</div>
<p>
If k=0, the probabilities are unsmoothed.  As k grows larger, the probabilities are 
smoothed more and more. You can use your validation set to determine a good value 
for k.  <strong>Note</strong>: don't smooth P(Y).

</p><p><em><strong>Question 1 (6 points)</strong></em>
Implement <code>trainAndTune</code> and <code>calculateLogJointProbabilities</code> in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/naiveBayes.html">naiveBayes.py</a></code>. 
In <code>trainAndTune</code>, estimate conditional probabilities from the training data for each possible value
of <em>k</em> given in the list <code>kgrid</code>.  
Evaluate accuracy on the held-out validation set for each <em>k</em> and choose
the value with the highest validation accuracy.  In case of ties,
prefer the <em>lowest</em> value of <em>k</em>.  Test your classifier with:

</p><pre>python dataClassifier.py -c naiveBayes --autotune </pre>



<p><strong>Hints and observations:</strong>
</p><ul>
    <li> The method <code>calculateLogJointProbabilities</code> uses the conditional probability tables constructed by 
<code>trainAndTune</code> to compute the log posterior probability for each label y given a feature vector. The comments of the method describe the data structures of the input and output.
    </li><li> You can add code to the <code>analysis</code> method in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/dataClassifier.html">dataClassifier.py</a></code> to explore the mistakes that your classifier is making.  This is optional.
    </li><li> When trying different values of the smoothing parameter <em>k</em>, think about the number of times you scan the training data. Your code should save computation by avoiding redundant reading.
    </li><li> To run your classifier with only one particular value of <em>k</em>, remove the <code>--autotune</code> option.  This will ensure that <code>kgrid</code> has only one value, which you can change with <code>-k</code>.
    </li><li> Using a fixed value of <em>k=2</em> and 100 training examples, you should get a validation accuracy of about 69% and a test accuracy of 55%.
    </li><li> Using <code>--autotune</code>, which tries different values of <em>k</em>, you should get a validation accuracy of about 74% and a test accuracy of 65%.
    </li><li> Accuracies may vary slightly because of implementation details.  For instance, ties are not deterministically
        broken in the <code>Counter.argMax()</code> method.
    </li><li> To run on the face recognition dataset, use <code>-d faces</code> (optional).
</li></ul>


<p></p><h4>Odds Ratios</h4>
One important tool in using classifiers in real domains is being able
to inspect what they have learned.  One way to inspect a naive Bayes
model is to look at the most likely features for a given label.

<p>
Another, better, tool for understanding the parameters is to look at <i>odds ratios</i>.  For each pixel
feature <img width="20" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img13.png" alt="$F_i$"> and classes <img width="41" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img23_new.png" alt="$y_1, y_2$">, consider the odds ratio:
<br></p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img24.png" alt="\begin{displaymath}
\mbox{odds}(F_i=on, y_1, y_2) = \frac{P(F_i=on\vert y_1)}{P(F_i=on\vert y_2)}
\end{displaymath}">
</div>
<br clear="ALL">
<p></p>
This ratio will be greater than one for features which cause belief in
<img width="19" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img25_new.png" alt="$y_1$"> to increase relative to <img width="19" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img26_new.png" alt="$y_2$">.

<p> The features that have the greatest impact at classification time are those with both a high
probability (because they appear often in the data) and a high odds ratio (because they strongly bias
one label versus another).

</p><p>To run the autograder for this question:
</p><pre> python autograder.py -q q1</pre>
<p><em><strong>Question 2 (2 points)</strong></em>
  Fill in the function <code>findHighOddsFeatures(self, label1, label2)</code>. 
  It should return a list of the 100 features with highest odds ratios for <code>label1</code>
  over <code>label2</code>.
  The option <code>-o</code> activates an odds ratio analysis.
  Use the options <code>-1 label1 -2 label2</code> to specify which labels to compare. Running the following command will show you the 100 pixels that best distinguish between a 3 and a 6.
  
</p><pre>python dataClassifier.py -a -d digits -c naiveBayes -o -1 3 -2 6  </pre>

Use what you learn from running this command to answer the following question. Which of the following images best shows those pixels
which have a high odds ratio with respect to 3 over 6? (That is, which of these is most like the output from the command you just ran?)
<center>
<table>
  <tbody><tr>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q2a.png"></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q2b.png"></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q2c.png"></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q2d.png"></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q2e.png"></td>
  </tr>
  <tr><td><center>(a)</center></td><td><center>(b)</center></td><td><center>(c)</center></td><td><center>(d)</center></td><td><center>(e)</center></td></tr>
</tbody></table>
</center>


<p><em>To answer:</em> please return 'a', 'b', 'c', 'd', or 'e' from the function <code>q2</code> in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/answers.html">answers.py</a></code>.</p>
<p><strong>Hints:</strong> </p>
<ul>
  <li> To avoid divide-zero erros you may want to use smoothing when computing the odds ratio.
</li></ul>
<h2>Perceptron</h2>
<br>
A skeleton implementation of a perceptron classifier is provided for
you in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code>. You will fill in the 
<code>train</code> function, and the <code>findHighWeightFeatures</code> function.

<p>
Unlike the naive Bayes classifier, a perceptron does not use
probabilities to make its decisions.  Instead, it keeps a
weight vector <img width="24" height="17" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img31_new.png" alt="$w^y$"> of each class <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$"> (<img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$"> is an identifier, not an exponent).  Given a feature list <img width="14" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img33.png" alt="$f$">,
the perceptron compute the class <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$"> whose weight vector is most similar
to the input vector <img width="14" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img33.png" alt="$f$">.  Formally, given a feature vector <img width="14" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img33.png" alt="$f$"> (in our case, a map from pixel locations to indicators of whether they are on), we score each class with:
<br></p><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img34.png" alt="\begin{displaymath}
\mbox{score}(f,y) = \sum_i f_i w^y_i
\end{displaymath}">
</div>
Then we choose the class with highest score as the predicted label for that data instance.
In the code, we will represent <img width="24" height="17" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img31_new.png" alt="$w^y$"> as a <code>Counter</code>.

<p></p><h4>Learning weights</h4>
In the
basic multi-class perceptron, we scan over the data, one instance at a
time.  When we come to an instance <img width="41" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img35.png" alt="$(f, y)$">, we find the label with highest score:
<div align="CENTER">
<img src="./Project 5_ Classification_files/img36.png" alt="\begin{displaymath}
y&#39; = \textmd{arg max}_{y&#39;&#39;} score(f,y&#39;&#39;)
\end{displaymath}">
</div>
<p></p>
We compare <img width="17" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img37.png" alt="$y&#39;$"> to the true label <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$">.  If <img width="47" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img38.png" alt="$y&#39; = y$">, we've gotten the
instance correct, and we do nothing.  Otherwise, we guessed <img width="17" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img37.png" alt="$y&#39;$"> but
we should have guessed <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$">.  That means that <img width="24" height="17" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img31_new.png" alt="$w^y$"> should have scored <img width="14" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img33.png" alt="$f$"> higher, and <img width="28" height="18" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img39.png" alt="$w^{y&#39;}$"> should have scored
<img width="14" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img33.png" alt="$f$"> lower, in order to prevent this error in the future.  We update these two weight vectors accordingly:
<br><p></p>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img40.png" alt="\begin{displaymath}
w^y += f
\end{displaymath}">
</div>
<div align="CENTER">
<img src="./Project 5_ Classification_files/img41.png" alt="\begin{displaymath}
w^{y&#39;} -= f
\end{displaymath}">
</div>
<p></p>

<p>
Using the addition, subtraction, and multiplication functionality of the
<code>Counter</code> class in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/util.html">util.py</a></code>, the perceptron updates should be
relatively easy to code.  Certain implementation issues have been
taken care of for you in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code>, such as handling iterations
over the training data and ordering the update trials.  Furthermore,
the code sets up the <code>weights</code> data structure for you.  Each
legal label needs its own <code>Counter</code> full of weights.

</p><p>

<em><strong>Question 3 (4 points)</strong></em>  Fill in the <code>train</code> method in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code>.  Run your code with:

</p><pre>python dataClassifier.py -c perceptron </pre>

<p><strong>Hints and observations:</strong>
</p><ul>
  <li> The command above should yield validation accuracies in the range between 40% to 70%
and test accuracy between 40% and 70% (with the default 3 iterations).  These ranges are wide because the perceptron is a lot more sensitive to the specific choice of tie-breaking than naive Bayes.
  </li><li> One of the problems with the perceptron is that its performance is sensitive to
several practical details, such as how many iterations you train it for, and the order you 
use for the training examples (in practice, using a randomized order works better
than a fixed order).  The current code uses a default value of 3 training iterations. You
can change the number of iterations for the perceptron with the <code>-i iterations</code>
option. Try different numbers of iterations and see how it influences the performance.
In practice, you would use the performance on the validation set to figure out
when to stop training, but you don't need to implement this stopping criterion for
this assignment.</li></ul>
  
    <p>To run the autograder for this question and visualize the output:</p>
  <pre> python autograder.py -q q3</pre>    


<p></p><h4>Visualizing weights</h4>
<p>Perceptron classifiers, and other discriminative methods, are often criticized 
  because the parameters they learn are hard to interpret.  To see a demonstration 
  of this issue, we can write a function to find features that are characteristic of
  one class. (Note that, because of the way perceptrons are trained, it is not
  as crucial to find odds ratios.)</p>
<p>
  <em><strong>Question 4 (1 point)</strong></em> Fill in <code>findHighWeightFeatures(self, label)</code> in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/perceptron.html">perceptron.py</a></code>. 
  It should return a list of the 100 features with highest weight for that label. You can display the 100 pixels with the largest weights 
  using the command:
  
</p><pre>python dataClassifier.py -c perceptron -w  </pre>

Use this command to look at the weights, and answer the following true/false question. Which of the following sequence of weights is
most representative of the perceptron?
<center>
<table>
  <tbody><tr>
    <td><b>(a)</b></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q4a0.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a1.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a2.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a3.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a4.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a5.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a6.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a7.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a8.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4a9.png"></td>
  </tr>
  <tr>
    <td><b>(b)</b></td>
    <td><img width="80" height="250" src="./Project 5_ Classification_files/q4b0.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b1.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b2.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b3.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b4.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b5.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b6.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b7.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b8.png"></td>
    <td><img width="81" height="250" src="./Project 5_ Classification_files/q4b9.png"></td>
  </tr>
</tbody></table>
</center>

Answer the question <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/answers.html">answers.py</a></code> in the method <code>q4</code>, returning either 'a' or 'b'. 



<h2>MIRA</h2>
A skeleton implementation of the MIRA classifier is provided for you in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/mira.html">mira.py</a></code>. MIRA is an online learner which is closely related to both the support vector machine and perceptron classifiers.  You will fill in the <code>trainAndTune</code> function.

<h4>Theory</h4>
Similar to a multi-class perceptron classifier, multi-class MIRA classifier also keeps a 
 weight vector <img width="24" height="17" align="BOTTOM" border="0" src="./Project 5_ Classification_files/img31_new.png" alt="$w^y$"> of each label <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$">.
     We also scan over the data, one instance at a
    time.  When we come to an instance <img width="41" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img35.png" alt="$(f, y)$">, we find the label with highest score:
    <div align="CENTER">
    <img src="./Project 5_ Classification_files/img36.png">
    </div>
    <p></p>
    We compare <img width="17" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img37.png" alt="$y&#39;$"> to the true label <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$">.  If <img width="47" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img38.png" alt="$y&#39; = y$">, we've gotten the
    instance correct, and we do nothing.  Otherwise, we guessed <img width="17" height="32" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img37.png" alt="$y&#39;$"> but
    we should have guessed <img width="13" height="30" align="MIDDLE" border="0" src="./Project 5_ Classification_files/img32_new.png" alt="$y$">. Unlike perceptron, we update the weight vectors of these labels with variable step size:
    <br><p></p>
    <div align="CENTER">
    <img src="./Project 5_ Classification_files/img59.png">
    </div>
    <div align="CENTER">
    <img src="./Project 5_ Classification_files/img60.png">     
    </div>
    where <img src="./Project 5_ Classification_files/img66.png"> is chosen such that it minimizes
    <div align="CENTER">
    <img src="./Project 5_ Classification_files/img61.png">
    </div>
    <div align="CENTER">
    
    subject to the condition that
    <img src="./Project 5_ Classification_files/img62.png" align="bottom">
     </div>
     <br>
     which is equivalent to 
     <div align="CENTER">
     <img src="./Project 5_ Classification_files/img63.png" align="middle">
     subject to 
     <img src="./Project 5_ Classification_files/img65.png" align="middle"> and 
      <img src="./Project 5_ Classification_files/img66.png" align="middle">
      </div>
    <p></p>
    Note that, <img src="./Project 5_ Classification_files/img68.png" align="middle">, so the condition <img src="./Project 5_ Classification_files/img66.png" align="middle"> is always true given <img src="./Project 5_ Classification_files/img65.png" align="middle"> Solving this simple problem, we then have
    <div align="CENTER">
     <img src="./Project 5_ Classification_files/img64.png">
     </div>
     However, we would like to cap the maximum possible value of <img src="./Project 5_ Classification_files/tau.png"> by a positive constant C, which leads us to 
     <div align="CENTER">
      <img src="./Project 5_ Classification_files/img67.png">
      </div>
<br>
<em><strong>Question 5 (6 points)</strong></em> Implement <code>trainAndTune</code> in <code><a href="http://www.cs.rpi.edu/~xial/Teaching/2018S/projects/classification/docs/mira.html">mira.py</a></code>.   
This method should train a MIRA classifier using each value of <em>C</em> in <code>Cgrid</code>.  
Evaluate accuracy on the held-out validation set for each <em>C</em> and choose
the <em>C</em> with the highest validation accuracy.  In case of ties,
prefer the <em>lowest</em> value of <em>C</em>.  Test your MIRA implementation with:

<pre>python dataClassifier.py -c mira --autotune </pre>

<p><strong>Hints and observations:</strong>
</p><ul> 
    <li>Pass through the data <code>self.max_iterations</code> times during training.
    </li><li>Store the weights learned using the best value of <em>C</em> at the end in <code>self.weights</code>, so that these weights can be used to test your classifier.
    </li><li>To use a fixed value of <em>C=0.001</em>, remove the <code>--autotune</code> option from the command above.  
    </li><li>Validation and test accuracy when using <code>--autotune</code> should be in the 60's.
    </li><li>It might save some debugging time if the +1 term above is implemented as +1.0, due to division truncation of integer arguments. Depending on how you implement this, it may not matter.</li>
	<li>The same code for returning high odds features in your perceptron implementation should also work for MIRA if you're curious what your classifier is learning.
</li></ul>


<p>To run the autograder for this question and visualize the output:</p>
<pre> python autograder.py -q q5</pre>
<p>
</p><p><em> Congratulations! You're finished all projects.</em>


</p></body></html>